import pandas as pd
import numpy as np
import xlrd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor,  RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFE
from pdpbox import pdp, get_dataset, info_plots
from sklearn.metrics import confusion_matrix, classification_report,precision_score, f1_score
import seaborn as sns
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler


loandata = pd.read_excel(r"C:\Users\fedey\Desktop\Bachelorarbeit\empirie\LoanData_bearbeitet.xlsx")
print(loandata.shape)
loandata = loandata.drop(columns=['Status'])

# replace any non-numeric values with NaN
loandata = loandata.apply(pd.to_numeric, errors='coerce')

# SimpleImputer object with the strategy 'median'
imputer = SimpleImputer(strategy='median')

# Fit the imputer to the data
imputer.fit(loandata)

#filling the missing values with the median
loandata = pd.DataFrame(imputer.transform(loandata), columns=loandata.columns)

loandata = loandata[['VerificationType',	'LanguageCode',	'Age',	'Gender',	'Country',	'AppliedAmount',	'Amount',	'Interest',	'LoanDuration',	'MonthlyPayment',	'UseOfLoan',	'Education',	'MaritalStatus',	'NrOfDependants',	'EmploymentStatus',	'EmploymentDurationCurrentEmployer',	'OccupationArea',	'HomeOwnershipType',	'IncomeTotal',	'ExistingLiabilities',	'LiabilitiesTotal'	,'DebtToIncome'	,'FreeCash','ExpectedLoss',	'LossGivenDefault', 	'ExpectedReturn',	'ProbabilityOfDefault'	,'Restructured'	,'NoOfPreviousLoansBeforeLoan'	,'AmountOfPreviousLoansBeforeLoan'	,'PreviousRepaymentsBeforeLoan',	'PreviousEarlyRepaymentsBefoleLoan'		,'Ruckzahlungsquote' ,'NewCreditCustomer','IncomeFromPrincipalEmployer','IncomeFromPension',	'IncomeFromFamilyAllowance',	'IncomeFromSocialWelfare',	'IncomeFromLeavePay',	'IncomeFromChildSupport',	'IncomeOther','PreviousEarlyRepaymentsCountBeforeLoan',	'RefinanceLiabilities'	,'CreditScoreEeMini']]
print(loandata.shape)
#Aussortierte Merkmale: Rating Amount Interest MonthlyPayment ExpectedLoss LossGivenDefault ExpectedReturn 'ProbabilityOfDefault'	,'Restructured','CreditScoreEeMini'
#Startwert eines Bereichs exklusiv und Endwert inklusiv
ranges = [(-30, 0.0),(0.0, 0.10), (0.10, 0.20), (0.20, 0.30), (0.30, 0.40), (0.40, 0.50), (0.50, 0.60), (0.60, 0.70), (0.70, 0.80), (0.80, 0.90), (0.90, 0.999999) ,(0.999999, 20)]
# Erstelle eine neue Spalte für die Range der Rückzahlungsquote
loandata['RepaymentRange'] = pd.cut(loandata['Ruckzahlungsquote'], bins=[range[0] for range in ranges] + [ranges[-1][-1]], labels=False)

x = loandata.drop(columns=['Ruckzahlungsquote', 'RepaymentRange'])
y = loandata['RepaymentRange']
#Test and TrainigData
x_train,x_test,y_train,y_test, = train_test_split(x,y, test_size = 0.2, random_state = 23)

#Random Forest-Klassifikator
rf = RandomForestClassifier(random_state=23,n_estimators = 1000)
#Ergebnisse durch gridsearch class_weight = class_weights,  max_depth = None, min_samples_leaf = 2, min_samples_split = 2, n_estimators= 1000 waren aber schlecht
print(x_train.columns)
rf.fit(x_train, y_train)

# Vorhersagen für Trainings- und Testdaten
y_pred_train = rf.predict(x_train)
y_pred_test = rf.predict(x_test)

# Bewertung der Vorhersagen
accuracy_train = accuracy_score(y_train, y_pred_train)
accuracy_test = accuracy_score(y_test, y_pred_test)

print("Training set accuracy:", accuracy_train)
print("Test set accuracy:", accuracy_test)

# Berechnung der Konfusionsmatrix training
conf_matrix1 = confusion_matrix(y_train, y_pred_train)
correct_predictions1 = np.diag(conf_matrix1)
number_of_correct_predictions1 = np.sum(correct_predictions1)
total_predictions1 = np.sum(conf_matrix1)
# Anzahl der korrekt zugeordneten Daten und die Genauigkeit in absoluten Werten
print("Number of correct predictions training:", number_of_correct_predictions1, total_predictions1)
# Heatmap der Konfusionsmatrix
sns.heatmap(conf_matrix1, annot=True, cmap='Blues')
plt.xlabel('Vorhergesagte Kategorie')
plt.ylabel('Echte Kategorie')
plt.title('Konfusionsmatrix')
plt.show()

# Berechnung der Konfusionsmatrix test
conf_matrix = confusion_matrix(y_test, y_pred_test)
# Diagonalelemente der Konfusionsmatrix
correct_predictions = np.diag(conf_matrix)
# Anzahl der korrekt zugeordneten Daten
number_of_correct_predictions = np.sum(correct_predictions)
# Gesamtanzahl der Vorhersagen
total_predictions = np.sum(conf_matrix)
# Anzahl der korrekt zugeordneten Daten und die Genauigkeit in absoluten Werten
print("Number of correct predictions test:", number_of_correct_predictions, total_predictions)
# Heatmap der Konfusionsmatrix
sns.heatmap(conf_matrix, annot=True, cmap='Blues')
plt.xlabel('Vorhergesagte Kategorie')
plt.ylabel('Echte Kategorie')
plt.title('Konfusionsmatrix')
plt.show()

#precision score
precision = precision_score(y_test, y_pred_test, average='weighted')
# F1 score
f1 = f1_score(y_test, y_pred_test, average='weighted')

# Print
print("Precision score:", precision)
print("F1 score:", f1)

#Klassifikationsbericht
report = classification_report(y_test, y_pred_test)
print(report)


#Tatsächliche vs Vorhergesagte Plot training
pred_counts = np.bincount(y_pred_train)
true_counts = np.bincount(y_train)

bar_width = 0.35
index = np.arange(len(pred_counts))
categories = ['0%', '0-10%', '10-20%', '20-30%',  '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-99,99%', '100%']

plt.bar(index, pred_counts, width=bar_width, label='Vorhergesagt')
plt.bar(index + bar_width, true_counts, width=bar_width, label='Tatsächlich')
plt.xlabel('Kategorie')
plt.ylabel('Anzahl')
plt.title('Vorhersagen vs. Tatsächliche Werte')
plt.xticks(index + bar_width / 2, categories[:len(pred_counts)])
plt.legend()
plt.show()


#Tatsächliche vs Vorhergesagte Plot test
pred_counts = np.bincount(y_pred_test)
true_counts = np.bincount(y_test)

bar_width = 0.35
index = np.arange(len(pred_counts))
categories = ['0%', '0-10%', '10-20%', '20-30%',  '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-99,99%', '100%']

plt.bar(index, pred_counts, width=bar_width, label='Vorhergesagt')
plt.bar(index + bar_width, true_counts, width=bar_width, label='Tatsächlich')
plt.xlabel('Kategorie')
plt.ylabel('Anzahl')
plt.title('Vorhersagen vs. Tatsächliche Werte')
plt.xticks(index + bar_width / 2, categories[:len(pred_counts)])
plt.legend()
plt.show()

#table with the importance of each feature
importance = rf.feature_importances_
features = x_train.columns
indices = np.argsort(importance)
plt.barh(range(x_train.shape[1]), importance[indices])
plt.yticks(range(x_train.shape[1]), features[indices])
plt.xlabel('Merkmalsrelevanz')
plt.ylabel('Merkmal')
plt.title('Merkmalsrelevanz')
plt.show()

#sagt wieviel jede merkmalsausprägung an bedeutung auf welche ausrichtung hat
features =['VerificationType',	'LanguageCode',	'Age',	'Gender',	'Country',	'AppliedAmount',	'Amount',	'Interest',	'LoanDuration',	'MonthlyPayment',	'UseOfLoan',	'Education',	'MaritalStatus',	'NrOfDependants',	'EmploymentStatus',	'EmploymentDurationCurrentEmployer',	'OccupationArea',	'HomeOwnershipType',	'IncomeTotal',	'ExistingLiabilities',	'LiabilitiesTotal'	,'DebtToIncome'	,'FreeCash','ExpectedLoss',	'LossGivenDefault', 	'ExpectedReturn',	'ProbabilityOfDefault'	,'Restructured'	,'NoOfPreviousLoansBeforeLoan'	,'AmountOfPreviousLoansBeforeLoan'	,'PreviousRepaymentsBeforeLoan',	'PreviousEarlyRepaymentsBefoleLoan'		,'NewCreditCustomer','IncomeFromPrincipalEmployer','IncomeFromPension',	'IncomeFromFamilyAllowance',	'IncomeFromSocialWelfare',	'IncomeFromLeavePay',	'IncomeFromChildSupport',	'IncomeOther','PreviousEarlyRepaymentsCountBeforeLoan',	'RefinanceLiabilities'	,'CreditScoreEeMini']

for feature in features:
    pdp_feature = pdp.pdp_isolate(model=rf, dataset=x, model_features=x.columns, feature=feature)
    pdp.pdp_plot(pdp_feature, feature)
    plt.show()
